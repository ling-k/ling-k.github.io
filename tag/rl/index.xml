<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RL | Lingzhi Kong</title>
    <link>https://example.com/tag/rl/</link>
      <atom:link href="https://example.com/tag/rl/index.xml" rel="self" type="application/rss+xml" />
    <description>RL</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright Â© 2021 Lingzhi Kong</copyright><lastBuildDate>Mon, 23 Aug 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0a5e3fe874455accc5deb05a8f5d73a3_87874_512x512_fill_lanczos_center_3.png</url>
      <title>RL</title>
      <link>https://example.com/tag/rl/</link>
    </image>
    
    <item>
      <title>Teaching an Artificial Agent to Play CarRacing Game</title>
      <link>https://example.com/project/rl_car_racing/</link>
      <pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/rl_car_racing/</guid>
      <description>&lt;p&gt;This project deals with decision making problems in autonomous driving by applying deep reinforcement
learning and behavioral cloning algorithm in a simulated car
racing environment. To be specific, we will implement deep
reinforcement learning algorithms (Proximal Policy Optimization) and behavioral cloning algorithm (DAGGER) and
use them to teach an agent car to play the CarRacing game.
As a deep reinforcement learning algorithm, Proximal
Policy Optimization directly optimizes the cumulative reward without using a value function, and it is able to be
trained directly with nonlinear function approximations such
as neural networks. Dataset Aggregation is an efficient way
to reproduce the expert demonstrated behavior. Expert data
will first be obtained by some well-trained expert. Then, the
state and actions made by the expert will be recorded at the
same time. Our goal for behavioral cloning is to teach an
agent to play the game using expert demonstrated data.&lt;/p&gt;
&lt;p&gt;As shown in the figure below, For traditional behavior cloning, we first collect expert demonstration data, and use it to learn a policy mapping from states to actions. However, there are some problems with this method. Expert only samples limited obersavations or, states, so the policy will make big mistakes when the agent goes to a state that has not been encountered before.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Car Image&#34; srcset=&#34;
               /project/rl_car_racing/scheme_dagger_hufae8384125843b1623a05055f27ff068_85307_2bcc1649b3a7eff4a315ff05a3ad71a6.png 400w,
               /project/rl_car_racing/scheme_dagger_hufae8384125843b1623a05055f27ff068_85307_22830dff6cf2ab0c15cb9011ff255e49.png 760w,
               /project/rl_car_racing/scheme_dagger_hufae8384125843b1623a05055f27ff068_85307_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://example.com/project/rl_car_racing/scheme_dagger_hufae8384125843b1623a05055f27ff068_85307_2bcc1649b3a7eff4a315ff05a3ad71a6.png&#34;
               width=&#34;574&#34;
               height=&#34;202&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

One solution for this problem is Dataset aggregration, or dagger.  The idea of dagger is that we use our learned policy to interact with the enviornment, and record the states and the action under expert policy. Then we add the new data to the dataset, and use the new dataset to train our new policy. It is just like iterative version of behavior cloning.&lt;/p&gt;
&lt;p&gt;More details: &lt;a href=&#34;https://ling-k.github.io/uploads/Kong_Xu_CS5180_Project_Report_edited.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;span style=&#34;color:blue&#34;&gt; Report &lt;/span&gt;&lt;/a&gt;, &lt;a href=&#34;https://ling-k.github.io/uploads/Kong_Xu_CS5180_Project_slides.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;span style=&#34;color:blue&#34;&gt; Slides &lt;/span&gt;&lt;/a&gt;,  &lt;a href=&#34;https://youtu.be/SGem0jixpic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;span style=&#34;color:blue&#34;&gt; Video &lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
