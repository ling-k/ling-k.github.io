<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Lingzhi Kong</title>
    <link>https://example.com/project/</link>
      <atom:link href="https://example.com/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Copyright Â© 2021 Lingzhi Kong</copyright><lastBuildDate>Mon, 23 Aug 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0a5e3fe874455accc5deb05a8f5d73a3_87874_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://example.com/project/</link>
    </image>
    
    <item>
      <title>Compositional Generalization in Object-Oriented Environments</title>
      <link>https://example.com/project/compositional_generaliaztion/</link>
      <pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/compositional_generaliaztion/</guid>
      <description>&lt;p&gt;We study compositional generalizaiton in world modeling, a key ability that humans possess. In particular, given scenes with some combinations of objects, if an agent has learned to interpret them as objects and their interactions, it should be able to generalize to scenes with novel combinations of objects.
We (1) formalize the compositional generalization problem with an algebraic approach and (2) study
how an agent can achieve that. We introduce (1) a synthetic and fully controllable
environment and (2) a principled empirical approach to systematically measure the
compositional generalization ability. Motivated by
the formulation, we emprically compare several methods using our environments
and design a straightforward approach, Homomorphic Object-oriented World
Model (HOWM), to study compositional generalization in dynamics prediction.&lt;/p&gt;
&lt;p&gt;More details: To appear, will be submitted to ICLR 2022.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Teaching an Artificial Agent to Play CarRacing Game</title>
      <link>https://example.com/project/rl_car_racing/</link>
      <pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/rl_car_racing/</guid>
      <description>&lt;p&gt;This project deals with decision making problems in autonomous driving by applying deep reinforcement
learning and behavioral cloning algorithm in a simulated car
racing environment. To be specific, we will implement deep
reinforcement learning algorithms (Proximal Policy Optimization) and behavioral cloning algorithm (DAGGER) and
use them to teach an agent car to play the CarRacing game.
As a deep reinforcement learning algorithm, Proximal
Policy Optimization directly optimizes the cumulative reward without using a value function, and it is able to be
trained directly with nonlinear function approximations such
as neural networks. Dataset Aggregation is an efficient way
to reproduce the expert demonstrated behavior. Expert data
will first be obtained by some well-trained expert. Then, the
state and actions made by the expert will be recorded at the
same time. Our goal for behavioral cloning is to teach an
agent to play the game using expert demonstrated data.&lt;/p&gt;
&lt;p&gt;As shown in the figure below, For traditional behavior cloning, we first collect expert demonstration data, and use it to learn a policy mapping from states to actions. However, there are some problems with this method. Expert only samples limited obersavations or, states, so the policy will make big mistakes when the agent goes to a state that has not been encountered before.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Car Image&#34; srcset=&#34;
               /project/rl_car_racing/scheme_dagger_hufae8384125843b1623a05055f27ff068_85307_2bcc1649b3a7eff4a315ff05a3ad71a6.png 400w,
               /project/rl_car_racing/scheme_dagger_hufae8384125843b1623a05055f27ff068_85307_22830dff6cf2ab0c15cb9011ff255e49.png 760w,
               /project/rl_car_racing/scheme_dagger_hufae8384125843b1623a05055f27ff068_85307_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://example.com/project/rl_car_racing/scheme_dagger_hufae8384125843b1623a05055f27ff068_85307_2bcc1649b3a7eff4a315ff05a3ad71a6.png&#34;
               width=&#34;574&#34;
               height=&#34;202&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

One solution for this problem is Dataset aggregration, or dagger.  The idea of dagger is that we use our learned policy to interact with the enviornment, and record the states and the action under expert policy. Then we add the new data to the dataset, and use the new dataset to train our new policy. It is just like iterative version of behavior cloning.&lt;/p&gt;
&lt;p&gt;More details: &lt;a href=&#34;https://ling-k.github.io/uploads/Kong_Xu_CS5180_Project_Report_edited.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;span style=&#34;color:blue&#34;&gt; Report &lt;/span&gt;&lt;/a&gt;, &lt;a href=&#34;https://ling-k.github.io/uploads/Kong_Xu_CS5180_Project_slides.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;span style=&#34;color:blue&#34;&gt; Slides &lt;/span&gt;&lt;/a&gt;,  &lt;a href=&#34;https://youtu.be/SGem0jixpic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;span style=&#34;color:blue&#34;&gt; Video &lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Image-Guided Surgical Robot</title>
      <link>https://example.com/project/surgical_robot/</link>
      <pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/surgical_robot/</guid>
      <description>&lt;p&gt;The conventional human-performed surgery of bone tumor resection has the drawbacks of imprecise, high labor intense on surgeons, longer recovery time. In this project, a surgical robot system for bone tumor resection is developed. First, the preoperative image of experimental bone is obtained by CT scanning. Then the surgeon is able to plan a surgery path by means of choosing key points on the 3D bone model. Second, the relationship between image space and surgical space is established. Finally the surgery path made by the surgeon in image space is mapped to surgical space and robot can receive the commands to operate the surgery. Meanwhile, the position of the surgical tool is displayed in image space in real time. Acetabular bone and thigh bone trials were performed at hospital and the outcome shows the feasibility of using robot to resect bone tumor.&lt;/p&gt;
&lt;p&gt;As shown in the figure below, the brief scheme of this project is to use the CT data of the bone to guide the robot to do the bone resection surgery.
We first get the CT data of the bone and reconstruct it to a 3D model. Then, we load the 3D reconstructed image of the bone into 3D slicer,  an open source software for research in image guided therapy, in which the surgeon could make a surgical path by choosing key points. Second we build the relationship between image space and surgical space. This is realized by ICP algorithm with two separated steps.
Finally, we map the surgery path planned by the surgeon from image space to surgical space and command the robot to operate the surgery. Meanwhile, the position of the surgery tool is displayed in image space.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Car Image&#34; srcset=&#34;
               /project/surgical_robot/Project-Scheme_hu010d76a8265ec93787ede896f7aaa38b_281524_1578e432ef9f77b533775e9a58ee4052.png 400w,
               /project/surgical_robot/Project-Scheme_hu010d76a8265ec93787ede896f7aaa38b_281524_b55b3de4b6670ba7f07032ff1ea59b0c.png 760w,
               /project/surgical_robot/Project-Scheme_hu010d76a8265ec93787ede896f7aaa38b_281524_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://example.com/project/surgical_robot/Project-Scheme_hu010d76a8265ec93787ede896f7aaa38b_281524_1578e432ef9f77b533775e9a58ee4052.png&#34;
               width=&#34;760&#34;
               height=&#34;424&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;More details: &lt;a href=&#34;https://ling-k.github.io/uploads/surgical_robot_project_slides.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;span style=&#34;color:blue&#34;&gt; Report &lt;/span&gt;&lt;/a&gt;, &lt;a href=&#34;https://ling-k.github.io/uploads/surgical_robot_project_slides.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;span style=&#34;color:blue&#34;&gt; Slides &lt;/span&gt;&lt;/a&gt;,  &lt;a href=&#34;https://www.youtube.com/watch?v=vR4T4orNpzU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;span style=&#34;color:blue&#34;&gt; Video &lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
